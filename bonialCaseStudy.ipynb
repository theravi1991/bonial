{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TQpTI2PtvcF",
        "outputId": "ee0c252f-f248-4829-dbfd-eda2a114f12f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.5.0 in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: prophet==1.1.5 in /usr/local/lib/python3.11/dist-packages (1.1.5)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.11/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scikit-learn==1.4.1.post1 in /usr/local/lib/python3.11/dist-packages (1.4.1.post1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark==3.5.0) (0.10.9.7)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet==1.1.5) (1.2.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from prophet==1.1.5) (3.10.0)\n",
            "Requirement already satisfied: holidays>=0.25 in /usr/local/lib/python3.11/dist-packages (from prophet==1.1.5) (0.69)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.11/dist-packages (from prophet==1.1.5) (4.67.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from prophet==1.1.5) (6.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.1.post1) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.1.post1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.4.1.post1) (3.6.0)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet==1.1.5) (0.5.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet==1.1.5) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Required packages & Install Dependencies\n",
        "!pip install pyspark==3.5.0 prophet==1.1.5 pandas==2.2.2 numpy==1.25.2 scikit-learn==1.4.1.post1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and Initiate Spark Session\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col, to_timestamp, to_date, count, when, lit, date_trunc\n",
        ")\n",
        "\n",
        "# Spark Session Initialization\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"RobotDelivery\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A5wNyj6qu6Qc"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Ingestion\n",
        "columns = [\n",
        "    \"ride_id\", \"rideable_type\", \"started_at\", \"ended_at\", \"start_station_name\",\n",
        "    \"start_station_id\", \"end_station_name\", \"end_station_id\",\n",
        "    \"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\", \"member_type\"\n",
        "]\n",
        "\n",
        "citibike_df = spark.read.csv(\"/content/drive/MyDrive/bonial/dataset/202502-citibike-tripdata_combined.csv\", header=False, inferSchema=True)\n",
        "citibike_df = citibike_df.toDF(*columns)\n",
        "\n",
        "holiday_df = spark.read.csv(\"/content/drive/MyDrive/bonial/us_holidays_2023.csv\", header=True, inferSchema=True)\n"
      ],
      "metadata": {
        "id": "gXi3HCFzu9By"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0B5_Tchwyc0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efe82ef1-1c34-40a9-ec1c-da5f7d29899d"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Quality Validations check and Logging.\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col, to_timestamp, count, when\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "output_data_path = \"/content/drive/MyDrive/bonial/dataset/outputFiles/validated_data\"\n",
        "dq_report_path = \"/content/drive/MyDrive/bonial/dataset/outputFiles/data_quality_report.csv\"\n",
        "\n",
        "os.makedirs(output_data_path, exist_ok=True)\n",
        "\n",
        "dq_report = []\n",
        "\n",
        "# Total record count once (expensive op)\n",
        "total_count = citibike_df.count()\n",
        "\n",
        "# Null value check\n",
        "for col_name in [\"started_at\", \"ended_at\"]:\n",
        "    null_count = citibike_df.filter(col(col_name).isNull()).count()\n",
        "    dq_report.append({\n",
        "        \"Check\": f\"Null Check - {col_name}\",\n",
        "        \"Total Records\": total_count,\n",
        "        \"Failed Records\": null_count,\n",
        "        \"Passed Records\": total_count - null_count,\n",
        "        \"Status\": \"PASS\" if null_count == 0 else \"FAIL\"\n",
        "    })\n",
        "\n",
        "# Timestamp conversion check\n",
        "citibike_df = citibike_df.withColumn(\"starttime\", to_timestamp(\"started_at\")) \\\n",
        "                         .withColumn(\"endtime\", to_timestamp(\"ended_at\"))\n",
        "\n",
        "invalid_starttime = citibike_df.filter(col(\"starttime\").isNull()).count()\n",
        "invalid_endtime = citibike_df.filter(col(\"endtime\").isNull()).count()\n",
        "\n",
        "dq_report += [\n",
        "    {\n",
        "        \"Check\": \"Timestamp Conversion - starttime\",\n",
        "        \"Total Records\": total_count,\n",
        "        \"Failed Records\": invalid_starttime,\n",
        "        \"Passed Records\": total_count - invalid_starttime,\n",
        "        \"Status\": \"PASS\" if invalid_starttime == 0 else \"FAIL\"\n",
        "    },\n",
        "    {\n",
        "        \"Check\": \"Timestamp Conversion - endtime\",\n",
        "        \"Total Records\": total_count,\n",
        "        \"Failed Records\": invalid_endtime,\n",
        "        \"Passed Records\": total_count - invalid_endtime,\n",
        "        \"Status\": \"PASS\" if invalid_endtime == 0 else \"FAIL\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Trip duration check (1s to 86400s)\n",
        "citibike_df = citibike_df.withColumn(\"tripduration\", col(\"endtime\").cast(\"long\") - col(\"starttime\").cast(\"long\"))\n",
        "valid_duration_df = citibike_df.filter((col(\"tripduration\") > 0) & (col(\"tripduration\") <= 86400))\n",
        "valid_duration_count = valid_duration_df.count()\n",
        "\n",
        "dq_report.append({\n",
        "    \"Check\": \"Trip Duration Range (1s to 86400s)\",\n",
        "    \"Total Records\": total_count,\n",
        "    \"Failed Records\": total_count - valid_duration_count,\n",
        "    \"Passed Records\": valid_duration_count,\n",
        "    \"Status\": \"PASS\" if valid_duration_count == total_count else \"FAIL\"\n",
        "})\n",
        "\n",
        "# Completeness check for key columns\n",
        "important_columns = [\"start_station_name\", \"end_station_name\", \"ride_id\"]\n",
        "for col_name in important_columns:\n",
        "    missing_count = citibike_df.filter(col(col_name).isNull()).count()\n",
        "    dq_report.append({\n",
        "        \"Check\": f\"Completeness - {col_name}\",\n",
        "        \"Total Records\": total_count,\n",
        "        \"Failed Records\": missing_count,\n",
        "        \"Passed Records\": total_count - missing_count,\n",
        "        \"Status\": \"PASS\" if missing_count == 0 else \"FAIL\"\n",
        "    })\n",
        "\n",
        "# Final Valid Records After DQ\n",
        "citibike_df = valid_duration_df\n",
        "citibike_df.write.mode(\"overwrite\").option(\"header\", True).csv(output_data_path)\n",
        "\n",
        "# Save DQ Report\n",
        "pd.DataFrame(dq_report).to_csv(dq_report_path, index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "oxROUsG7u_0y"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Holiday Trips\n",
        "from pyspark.sql.functions import to_date, col\n",
        "\n",
        "holiday_df = holiday_df.withColumn(\"holiday_date\", to_date(col(\"HolidayDate\"), \"dd-MM-yyyy\"))\n",
        "\n",
        "holiday_df.select(\"HolidayDate\", \"holiday_date\").show(5)\n",
        "\n",
        "citibike_df = citibike_df.withColumn(\"trip_date\", to_date(col(\"started_at\")))\n",
        "\n",
        "joined_df = citibike_df.join(\n",
        "    holiday_df,\n",
        "    citibike_df.trip_date == holiday_df.holiday_date,\n",
        "    \"left\"\n",
        ").withColumn(\"is_holiday\", col(\"HolidayName\").isNotNull())\n",
        "\n",
        "# Save to CSV and Parquet\n",
        "output_path = \"/content/drive/MyDrive/bonial/dataset/outputFiles/holiday_trip_data\"\n",
        "joined_df.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{output_path}/csv\")\n",
        "joined_df.write.mode(\"overwrite\").parquet(f\"{output_path}/parquet\")\n",
        "\n",
        "total_trips = joined_df.count()\n",
        "holiday_trips = joined_df.filter(col(\"is_holiday\") == True).count()\n",
        "\n",
        "print(f\"Total trips: {total_trips}\")\n",
        "print(f\"Holiday trips found: {holiday_trips}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoUajCvcAVhz",
        "outputId": "6e9e5496-30dc-47af-e170-2a700a56ef41"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|HolidayDate|holiday_date|\n",
            "+-----------+------------+\n",
            "| 01-01-2025|  2025-01-01|\n",
            "| 16-01-2025|  2025-01-16|\n",
            "| 20-02-2025|  2025-02-20|\n",
            "| 29-05-2025|  2025-05-29|\n",
            "| 19-06-2025|  2025-06-19|\n",
            "+-----------+------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Total trips: 2030942\n",
            "Holiday trips found: 58366\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Demand Forecasting for the upcoming trips\n",
        "\n",
        "import os\n",
        "from prophet import Prophet\n",
        "from pyspark.sql.functions import date_trunc, count\n",
        "\n",
        "# Step 1: Aggregate trips per hour\n",
        "hourly_df = joined_df.groupBy(\n",
        "    date_trunc(\"hour\", \"starttime\").alias(\"ds\")\n",
        ").agg(count(\"*\").alias(\"y\"))\n",
        "\n",
        "# Step 2: Convert to pandas\n",
        "hourly_pd = hourly_df.toPandas()\n",
        "\n",
        "# Step 3: Run Forecast only if we have data\n",
        "if not hourly_pd.empty:\n",
        "    model = Prophet()\n",
        "    model.fit(hourly_pd)\n",
        "    future = model.make_future_dataframe(periods=24, freq=\"h\")\n",
        "    forecast = model.predict(future)\n",
        "    forecast_output = forecast[['ds', 'yhat']]\n",
        "\n",
        "    # Step 4: Save\n",
        "    forecast_output_path = \"/content/drive/MyDrive/bonial/dataset/outputFiles/forecast_data\"\n",
        "    os.makedirs(forecast_output_path, exist_ok=True)\n",
        "\n",
        "    forecast_output.to_csv(f\"{forecast_output_path}/forecast_output.csv\", index=False)\n",
        "    forecast_output.to_parquet(f\"{forecast_output_path}/forecast_output.parquet\", index=False)\n",
        "\n",
        "    print(\"Forecast saved.\")\n",
        "else:\n",
        "    print(\"No data available for forecasting.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cVq6Wh1_vDfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cda5b2f-88ce-45a6-b009-b694f34a0ce8"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpylid9w0o/161d8lg8.json\n",
            "DEBUG:cmdstanpy:input tempfile: /tmp/tmpylid9w0o/cpl0ptm9.json\n",
            "DEBUG:cmdstanpy:idx 0\n",
            "DEBUG:cmdstanpy:running CmdStan, num_threads: None\n",
            "DEBUG:cmdstanpy:CmdStan args: ['/usr/local/lib/python3.11/dist-packages/prophet/stan_model/prophet_model.bin', 'random', 'seed=22362', 'data', 'file=/tmp/tmpylid9w0o/161d8lg8.json', 'init=/tmp/tmpylid9w0o/cpl0ptm9.json', 'output', 'file=/tmp/tmpylid9w0o/prophet_model8ce_wqnb/prophet_model-20250321151550.csv', 'method=optimize', 'algorithm=lbfgs', 'iter=10000']\n",
            "15:15:50 - cmdstanpy - INFO - Chain [1] start processing\n",
            "INFO:cmdstanpy:Chain [1] start processing\n",
            "15:15:50 - cmdstanpy - INFO - Chain [1] done processing\n",
            "INFO:cmdstanpy:Chain [1] done processing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forecast saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate robot data\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "robot_pd = pd.DataFrame({\n",
        "    \"robot_id\": range(10),\n",
        "    \"charge_level\": np.random.randint(10, 100, size=10),\n",
        "    \"location\": [f\"area_{random.randint(1, 5)}\" for _ in range(10)]\n",
        "})\n",
        "\n",
        "# Ensure at least one robot in area_1 is eligible\n",
        "if \"area_1\" not in robot_pd[\"location\"].values:\n",
        "    random_index = random.randint(0, 9)\n",
        "    robot_pd.loc[random_index, \"location\"] = \"area_1\"\n",
        "\n",
        "area_1_indices = robot_pd.index[robot_pd[\"location\"] == \"area_1\"].tolist()\n",
        "if area_1_indices:\n",
        "    robot_pd.loc[random.choice(area_1_indices), \"charge_level\"] = random.randint(51, 100)\n",
        "\n",
        "robot_df = spark.createDataFrame(robot_pd)\n",
        "\n",
        "# Save\n",
        "output_path = \"/content/drive/MyDrive/bonial/dataset/outputFiles/robot_data\"\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "robot_df.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{output_path}/csv\")\n",
        "robot_df.write.mode(\"overwrite\").parquet(f\"{output_path}/parquet\")\n",
        "\n",
        "print(f\" Simulated Robot Data Saved with {robot_df.count()} records.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "34XLw9w9vFu7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b644af86-9ba1-4c91-9fec-e58013db18e9"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Simulated Robot Data Saved with 10 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Receipts Generation\n",
        "from pyspark.sql.functions import col\n",
        "import os\n",
        "\n",
        "receipts_df = joined_df.selectExpr(\n",
        "    \"ride_id as delivery_id\",\n",
        "    \"starttime as delivery_time\",\n",
        "    \"start_station_id as from_location\",\n",
        "    \"end_station_id as to_location\"\n",
        ").withColumn(\"receipt\", col(\"delivery_id\"))\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/bonial/dataset/outputFiles/receipts\"\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "receipts_df.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{output_path}/csv\")\n",
        "receipts_df.write.mode(\"overwrite\").parquet(f\"{output_path}/parquet\")\n",
        "\n",
        "print(f\" Receipts Generated: {receipts_df.count()} records.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fn0YSCgQvLBC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21fb97f2-40b9-4869-ffae-5d0f0c2650ca"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Receipts Generated: 2030942 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Inter-City Trip logs\n",
        "inter_city_df = joined_df.filter(col(\"start_station_id\") != col(\"end_station_id\"))\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/bonial/dataset/outputFiles/inter_city_trips\"\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "inter_city_df.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{output_path}/csv\")\n",
        "inter_city_df.write.mode(\"overwrite\").parquet(f\"{output_path}/parquet\")\n",
        "\n",
        "print(f\" Inter-City Trips Saved: {inter_city_df.count()} records.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "AAioancTvN1S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c531b98-8c06-490d-9dd2-3b7ed2d3c4ea"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Inter-City Trips Saved: 1998691 records.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Robots availability for pickup - Already charged and available in specific area\n",
        "\n",
        "available_df = robot_df.filter(col(\"charge_level\") > 50)\n",
        "\n",
        "output_path = \"/content/drive/MyDrive/bonial/dataset/outputFiles/available_robots\"\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "available_df.write.mode(\"overwrite\").option(\"header\", True).csv(f\"{output_path}/csv\")\n",
        "available_df.write.mode(\"overwrite\").parquet(f\"{output_path}/parquet\")\n",
        "\n",
        "print(f\" Available Robots Saved: {available_df.count()} records.\")\n"
      ],
      "metadata": {
        "id": "W47LIExkvToS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdaf1be4-cd4f-4920-8162-c9207889d9e8"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Available Robots Saved: 8 records.\n"
          ]
        }
      ]
    }
  ]
}